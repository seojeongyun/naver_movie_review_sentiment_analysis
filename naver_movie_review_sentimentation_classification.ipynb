{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMk+ZGgI3oGG42Dj31srY0I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seojeongyun/naver_movie_review_sentiment_analysis/blob/main/naver_movie_review_sentimentation_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGHAjc6mko8X",
        "outputId": "c6cbd880-0592-4196-941b-e997395559ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd \"/content/drive/My Drive/ratings_train.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ9fydY0k3hU",
        "outputId": "8743eb36-36c5-4492-bc29-4e1aaa0d42a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 20] Not a directory: '/content/drive/My Drive/ratings_train.txt'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/seojeongyun/naver_movie_review_sentiment_analysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOyCFIPOk3oP",
        "outputId": "c77fc28d-0aad-454f-8d0f-10fd31a805a4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'naver_movie_review_sentiment_analysis'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 22 (delta 3), reused 12 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (22/22), 8.09 MiB | 1.52 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Data download from url\n",
        "    urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\",\n",
        "                               filename=\"/content/drive/My Drive/ratings_train.txt\")\n",
        "    urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\",\n",
        "                               filename=\"/content/drive/My Drive/ratings_test.txt\")"
      ],
      "metadata": {
        "id": "8iMWdTA2k3qi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEtOVHR8mZk0",
        "outputId": "a3900160-0ba4-4674-d7b3-01b167111129"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import urllib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "class preprocess:\n",
        "    def __init__(self):\n",
        "        # By using pandas, data save\n",
        "        self.train_data = pd.read_table('/content/drive/My Drive/ratings_train.txt')\n",
        "        self.test_data = pd.read_table('/content/drive/My Drive/ratings_test.txt')\n",
        "        self.stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다']\n",
        "        self.Okt = Okt()\n",
        "\n",
        "    def dataset_len(self, type: str):  # Check the number of datasets\n",
        "        if type == 'train':\n",
        "            print('The number of train reviews : ', len(self.train_data))\n",
        "        else:\n",
        "            print('The number of test reviews : ', len(self.test_data))\n",
        "\n",
        "    def process(self, type: str):\n",
        "        if type == 'train':\n",
        "            # Remove duplications column of document\n",
        "            self.train_data.drop_duplicates(subset=['document'], inplace=True)\n",
        "\n",
        "            # Remove the samples with null value\n",
        "            train_data = self.train_data.dropna(how='any')\n",
        "\n",
        "            # Remove special characters with regular expression\n",
        "            train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")\n",
        "            train_data[:5]\n",
        "\n",
        "            # Change the white space value to Null value and then remove\n",
        "            train_data['document'] = train_data['document'].str.replace('^ +', \"\")  # change the white space to empty value\n",
        "            train_data['document'].replace('', np.nan, inplace=True)\n",
        "            print(train_data.isnull().sum())\n",
        "            train_data = train_data.dropna(how='any')\n",
        "\n",
        "            y_train = np.array(train_data['label']) # get labels\n",
        "            return train_data, y_train\n",
        "\n",
        "        else:\n",
        "            # Apply same preprocess to test dataset\n",
        "            self.test_data.drop_duplicates(subset=['document'], inplace=True)\n",
        "\n",
        "            # Remove the samples with null value\n",
        "            test_data = self.train_data.dropna(how='any')\n",
        "\n",
        "            # remove duplicate value for column of ducument\n",
        "            test_data['document'] = self.test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")  # Apply regular expression\n",
        "            test_data[:5]\n",
        "\n",
        "            test_data['document'] = self.test_data['document'].str.replace('^ +', \"\")  # change the white space to empty value\n",
        "            test_data['document'].replace('', np.nan, inplace=True)  # change epmty space to null value\n",
        "            test_data = test_data.dropna(how='any')  # remove the null value\n",
        "            y_test = np.array(test_data['label'])\n",
        "            return test_data, y_test\n",
        "\n",
        "    def check_duplication(self):  # Check duplications column of document and label\n",
        "        self.train_data['document'].nunique(), self.train_data['label'].nunique()\n",
        "        self.test_data['document'].nunique(), self.test_data['label'].nunique()\n",
        "\n",
        "    def remove_stopword(self, type: str, data):\n",
        "        if type == 'train':\n",
        "            X_train = []\n",
        "            for sentence in tqdm(data['document']):\n",
        "                tokenized_sentence = (self.Okt.morphs(sentence, stem=True))  # 토큰화\n",
        "                stopwords_removed_sentence = [word for word in tokenized_sentence if not word in self.stopwords]  # 불용어 제거\n",
        "                X_train.append(stopwords_removed_sentence)\n",
        "            return X_train\n",
        "\n",
        "        else:\n",
        "            X_test = []\n",
        "            for sentence in tqdm(data['document']):\n",
        "                tokenized_sentence = self.Okt.morphs(sentence, stem=True)  # 토큰화\n",
        "                stopwords_removed_sentence = [word for word in tokenized_sentence if not word in self.stopwords]  # 불용어 제거\n",
        "                X_test.append(stopwords_removed_sentence)\n",
        "            return X_test\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     data = preprocess()\n",
        "\n",
        "#     data.dataset_len('train')\n",
        "#     data.dataset_len('test')\n",
        "\n",
        "#     data.check_duplication()\n",
        "\n",
        "#     data.process('train')\n",
        "#     data.process('test')\n",
        "\n",
        "#     # Check the ratio of train and labels\n",
        "#     data.train_data['label'].value_counts()\n",
        "\n",
        "#     # Check the null value from train set\n",
        "#     print(data.train_data.isnull().values.any())\n",
        "#     print(data.train_data.isnull().sum())\n",
        "#     data.train_data.loc[data.train_data.document.isnull()]\n",
        "\n",
        "#     print('The number of test dataset after preprocess :',len(data.test_data))"
      ],
      "metadata": {
        "id": "ytSDWI3oliad"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "sf2Irfm_pHid"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize\n",
        "class Tokenize:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = Tokenizer()\n",
        "        self.stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다']\n",
        "        self.threshold = 3\n",
        "        self.rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "        self.total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "        self.rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "        self.vocab_size = 0\n",
        "    def Tokenizer(self, data):\n",
        "        return self.tokenizer.fit_on_texts(data)\n",
        "\n",
        "\n",
        "    def pairing(self, data):\n",
        "        self.tokenizer.fit_on_texts(data)\n",
        "        self.total_cnt = len(self.tokenizer.word_index) # 단어의 수\n",
        "        # 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "        for key, value in self.tokenizer.word_counts.items():\n",
        "            self.total_freq = self.total_freq + value\n",
        "\n",
        "            # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "            if(value < self.threshold):\n",
        "                self.rare_cnt = self.rare_cnt + 1\n",
        "                self.Tokenizerrare_freq = self.rare_freq + value\n",
        "\n",
        "        print('단어 집합(vocabulary)의 크기 :',self.total_cnt)\n",
        "        print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(self.threshold - 1, self.rare_cnt))\n",
        "        print(\"단어 집합에서 희귀 단어의 비율:\", (self.rare_cnt / self.total_cnt)*100)\n",
        "        print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (self.rare_freq / self.total_freq)*100)\n",
        "\n",
        "        ### 등장 빈도수 보다 작으면 제거\n",
        "        self.vocab_size = self.total_cnt - self.rare_cnt + 1\n",
        "        print('단어 집합의 크기 :',self.vocab_size)\n",
        "\n",
        "        return self.vocab_size\n",
        "\n",
        "        #\n",
        "    def text2int(self, data, type):\n",
        "        tokenizer = Tokenizer(self.vocab_size)\n",
        "        print('vocabsize로 토크나이저설정')\n",
        "        tokenizer.fit_on_texts(data)\n",
        "        print('핏온텍스트')\n",
        "        if type=='train':\n",
        "            X_train = tokenizer.texts_to_sequences(data)\n",
        "            print('트레인 : 텍스트 투 시퀀스')\n",
        "            print(X_train)\n",
        "            return X_train\n",
        "        else:\n",
        "            X_test = tokenizer.texts_to_sequences(data)\n",
        "            return X_test\n",
        "\n",
        "def below_threshold_len(max_len, nested_list):\n",
        "  count = 0\n",
        "  for sentence in nested_list:\n",
        "    if(len(sentence) <= max_len):\n",
        "        count = count + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))"
      ],
      "metadata": {
        "id": "SUifZSxuk3sh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    ### 클래스 선언\n",
        "    token = Tokenize()\n",
        "    data = preprocess()"
      ],
      "metadata": {
        "id": "xMNahejrGRSu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    ### 데이터 전처리\n",
        "    X_train, Y_train = data.process('train')\n",
        "    X_test, Y_test = data.process('test')\n",
        "\n",
        "    ## 불용어 제거\n",
        "    X_train_removed = data.remove_stopword('train', X_train)\n",
        "    X_test_removed = data.remove_stopword('test', X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_phtLo0DzrRa",
        "outputId": "fb9c5409-aaa1-404f-934d-6e98dbf19940"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-4fa79e7dfdbd>:35: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")\n",
            "<ipython-input-7-4fa79e7dfdbd>:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")\n",
            "<ipython-input-7-4fa79e7dfdbd>:39: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train_data['document'] = train_data['document'].str.replace('^ +', \"\")  # change the white space to empty value\n",
            "<ipython-input-7-4fa79e7dfdbd>:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data['document'] = train_data['document'].str.replace('^ +', \"\")  # change the white space to empty value\n",
            "<ipython-input-7-4fa79e7dfdbd>:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data['document'].replace('', np.nan, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id            0\n",
            "document    789\n",
            "label         0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-4fa79e7dfdbd>:55: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  test_data['document'] = self.test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")  # Apply regular expression\n",
            "<ipython-input-7-4fa79e7dfdbd>:55: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data['document'] = self.test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")  # Apply regular expression\n",
            "<ipython-input-7-4fa79e7dfdbd>:58: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  test_data['document'] = self.test_data['document'].str.replace('^ +', \"\")  # change the white space to empty value\n",
            "<ipython-input-7-4fa79e7dfdbd>:58: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data['document'] = self.test_data['document'].str.replace('^ +', \"\")  # change the white space to empty value\n",
            "<ipython-input-7-4fa79e7dfdbd>:59: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data['document'].replace('', np.nan, inplace=True)  # change epmty space to null value\n",
            "100%|██████████| 145393/145393 [05:33<00:00, 436.61it/s]\n",
            "100%|██████████| 48297/48297 [01:32<00:00, 521.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "# ### 토큰화 및 정수 인코딩\n",
        "    # train_token = token.Tokenizer(X_train)\n",
        "    # test_token = token.Tokenizer(X_test)\n",
        "\n",
        "    ### 등장 빈도 수 낮은 단어 제외\n",
        "    vocab_size = token.pairing(X_train_removed)\n",
        "    print(len(X_train_removed))\n",
        "    print('###############################')\n",
        "\n",
        "    ### text2sequence\n",
        "    train_token = token.text2int(data=X_train_removed, type='train')\n",
        "    test_token = token.text2int(data=X_test_removed, type='test')\n",
        "    print(train_token)\n",
        "    print('###############################')\n",
        "\n",
        "    ### remove empty sample\n",
        "    drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
        "    x_train = np.delete(train_token, drop_train, axis=0)\n",
        "    y_train = np.delete(Y_train, drop_train, axis=0)\n",
        "\n",
        "    ### padding\n",
        "    max_len = 30\n",
        "    below_threshold_len(max_len, x_train)\n",
        "    X_train = pad_sequences(x_train, maxlen=max_len)\n",
        "    X_test = pad_sequences(test_token, maxlen=max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAY1EhfOzgEZ",
        "outputId": "cd79ee70-e3a6-4138-9581-622dfaeb9ad0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합(vocabulary)의 크기 : 43752\n",
            "등장 빈도가 2번 이하인 희귀 단어의 수: 24337\n",
            "단어 집합에서 희귀 단어의 비율: 55.62488571950996\n",
            "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 0.0\n",
            "단어 집합의 크기 : 19416\n",
            "145393\n",
            "###############################\n",
            "vocabsize로 토크나이저설정\n",
            "핏온텍스트\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "핏온텍스트\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:5071: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = asarray(arr)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch"
      ],
      "metadata": {
        "id": "-OLf9M-Vx80s"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim, embedding_dim, dropout):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=0)\n",
        "    self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, dropout=dropout)\n",
        "    self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.sigmoid = torch.sigmoid\n",
        "\n",
        "  def forward(self, x):\n",
        "    embed = self.dropout(self.embedding(x))\n",
        "    output, _ = self.rnn(embed)\n",
        "    output = self.linear(output[:, -1, :])\n",
        "    return output\n",
        "\n",
        "  def predict(self, output):\n",
        "    prob = self.sigmoid(output)\n",
        "    prob[prob > 0.5] = 1\n",
        "    prob[prob <= 0.5] = 0\n",
        "    return prob\n",
        "\n",
        "  def _init_state(self, batch_size=1):\n",
        "    weight = next(self.parameters()).data\n",
        "    return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()"
      ],
      "metadata": {
        "id": "IsyhnNoYvq09"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "dropout = 0.3\n",
        "batch_size = 5\n",
        "\n",
        "model = LSTM(input_dim=vocab_size, embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_dim=1, dropout=dropout)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCEWithLogitsLoss()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McYvBMPnx2iS",
        "outputId": "f4edc4f7-90d2-4301-a654-363d167ebd98"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ko_vector(Dataset):\n",
        "    def __init__(self, x_train, y_train):\n",
        "        self.x = x_train\n",
        "        self.y = y_train.astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index], self.y[index]"
      ],
      "metadata": {
        "id": "JcatZa5B1CVq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "ko_object = ko_vector(X_train, Y_train)\n",
        "train_loader = DataLoader(ko_object,\n",
        "                          batch_size=5,\n",
        "                          shuffle=True)\n"
      ],
      "metadata": {
        "id": "vuTHfGdf2CI0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(pred, target):\n",
        "    return torch.sum(pred == target).item()\n",
        "\n",
        "\n",
        "def train(model, train_loader):\n",
        "  model.train()\n",
        "\n",
        "  epoch_loss, epoch_acc = 0, 0\n",
        "  for i, batch_data in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    x, y = batch_data[0], batch_data[1]\n",
        "    y_hat = model(x).squeeze(1)\n",
        "\n",
        "    loss = criterion(y_hat, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    pred = model.predict(y_hat)\n",
        "    epoch_acc += accuracy(pred, y)\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  return epoch_loss / (len(train_loader)*5), epoch_acc / (len(train_loader)*5)"
      ],
      "metadata": {
        "id": "v2SGoCTRxWre"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = []\n",
        "train_acc = []\n",
        "\n",
        "for _epoch in range(5): # 5 epoch\n",
        "  print(\"start {} epoch\".format(_epoch))\n",
        "  loss, acc = train(model, train_loader)\n",
        "  train_loss.append(loss)\n",
        "  train_acc.append(acc)"
      ],
      "metadata": {
        "id": "ZU5miP_c0Z6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "GX3199cQv1Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(list(range(5)), train_loss)\n",
        "plt.xticks(list(range(5)), list(range(5)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "17iwkbCVzEOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(list(range(5)), train_acc)\n",
        "plt.xticks(list(range(5)), list(range(5)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eBE_8NF8-KIv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}