{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMDhe7nYr9EGDULyM5dhDGG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seojeongyun/naver_movie_review_sentiment_analysis/blob/main/naver_movie_review_sentimentation_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGHAjc6mko8X",
        "outputId": "95535e28-bad5-47fd-cd4e-9e7491eb2851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd \"/content/drive/My Drive/ratings_train.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ9fydY0k3hU",
        "outputId": "cd3bd240-898e-4291-9cfd-aea6d7e1c7bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 20] Not a directory: '/content/drive/My Drive/ratings_train.txt'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/seojeongyun/naver_movie_review_sentiment_analysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOyCFIPOk3oP",
        "outputId": "4c3d2b4d-e73c-4be3-83d2-05ad5d4d397d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'naver_movie_review_sentiment_analysis'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 16 (delta 1), reused 12 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (16/16), 8.06 MiB | 13.96 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Data download from url\n",
        "    urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\",\n",
        "                               filename=\"/content/drive/My Drive/ratings_train.txt\")\n",
        "    urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\",\n",
        "                               filename=\"/content/drive/My Drive/ratings_test.txt\")"
      ],
      "metadata": {
        "id": "8iMWdTA2k3qi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEtOVHR8mZk0",
        "outputId": "298f4ae5-a950-473c-a589-1d7176c20e0a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import urllib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from konlpy.tag import Mecab\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "class preprocess:\n",
        "    def __init__(self):\n",
        "        # By using pandas, data save\n",
        "        self.train_data = pd.read_table('/content/drive/My Drive/ratings_train.txt')\n",
        "        self.test_data = pd.read_table('/content/drive/My Drive/ratings_test.txt')\n",
        "        self.stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다']\n",
        "        self.Okt = Okt()\n",
        "\n",
        "    def dataset_len(self, type: str):  # Check the number of datasets\n",
        "        if type == 'train':\n",
        "            print('The number of train reviews : ', len(self.train_data))\n",
        "        else:\n",
        "            print('The number of test reviews : ', len(self.test_data))\n",
        "\n",
        "    def process(self, type: str):\n",
        "        if type == 'train':\n",
        "            # Remove duplications column of document\n",
        "            self.train_data.drop_duplicates(subset=['document'], inplace=True)\n",
        "\n",
        "            # Remove the samples with null value\n",
        "            train_data = self.train_data.dropna(how='any')\n",
        "\n",
        "            # Remove special characters with regular expression\n",
        "            train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")\n",
        "            train_data[:5]\n",
        "\n",
        "            # Change the white space value to Null value and then remove\n",
        "            train_data['document'] = train_data['document'].str.replace('^ +', \"\")  # change the white space to empty value\n",
        "            train_data['document'].replace('', np.nan, inplace=True)\n",
        "            print(train_data.isnull().sum())\n",
        "            train_data = train_data.dropna(how='any')\n",
        "\n",
        "            y_train = np.array(train_data['label']) # get labels\n",
        "            return train_data, y_train\n",
        "\n",
        "        else:\n",
        "            # Apply same preprocess to test dataset\n",
        "            self.test_data.drop_duplicates(subset=['document'], inplace=True)\n",
        "\n",
        "            # Remove the samples with null value\n",
        "            test_data = self.train_data.dropna(how='any')\n",
        "\n",
        "            # remove duplicate value for column of ducument\n",
        "            test_data['document'] = self.test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")  # Apply regular expression\n",
        "            test_data[:5]\n",
        "\n",
        "            test_data['document'] = self.test_data['document'].str.replace('^ +', \"\")  # change the white space to empty value\n",
        "            test_data['document'].replace('', np.nan, inplace=True)  # change epmty space to null value\n",
        "            test_data = test_data.dropna(how='any')  # remove the null value\n",
        "            y_test = np.array(test_data['label'])\n",
        "            return test_data, y_test\n",
        "\n",
        "    def check_duplication(self):  # Check duplications column of document and label\n",
        "        self.train_data['document'].nunique(), self.train_data['label'].nunique()\n",
        "        self.test_data['document'].nunique(), self.test_data['label'].nunique()\n",
        "\n",
        "    def remove_stopword(self, type: str, data):\n",
        "        if type == 'train':\n",
        "            X_train = []\n",
        "            for sentence in tqdm(data['document']):\n",
        "                tokenized_sentence = (self.Okt.morphs(sentence, stem=True))  # 토큰화\n",
        "                stopwords_removed_sentence = [word for word in tokenized_sentence if not word in self.stopwords]  # 불용어 제거\n",
        "                X_train.append(stopwords_removed_sentence)\n",
        "            return X_train\n",
        "\n",
        "        else:\n",
        "            X_test = []\n",
        "            for sentence in tqdm(data['document']):\n",
        "                tokenized_sentence = self.Okt.morphs(sentence, stem=True)  # 토큰화\n",
        "                stopwords_removed_sentence = [word for word in tokenized_sentence if not word in self.stopwords]  # 불용어 제거\n",
        "                X_test.append(stopwords_removed_sentence)\n",
        "            return X_test\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data = preprocess()\n",
        "\n",
        "    data.dataset_len('train')\n",
        "    data.dataset_len('test')\n",
        "\n",
        "    data.check_duplication()\n",
        "\n",
        "    data.process('train')\n",
        "    data.process('test')\n",
        "\n",
        "    # Check the ratio of train and labels\n",
        "    data.train_data['label'].value_counts()\n",
        "\n",
        "    # Check the null value from train set\n",
        "    print(data.train_data.isnull().values.any())\n",
        "    print(data.train_data.isnull().sum())\n",
        "    data.train_data.loc[data.train_data.document.isnull()]\n",
        "\n",
        "    print('The number of test dataset after preprocess :',len(data.test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytSDWI3oliad",
        "outputId": "502dc095-98a7-4541-dda1-fae024287592"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of train reviews :  150000\n",
            "The number of test reviews :  50000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-9d4d4d064f29>:36: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")\n",
            "<ipython-input-8-9d4d4d064f29>:36: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")\n",
            "<ipython-input-8-9d4d4d064f29>:40: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train_data['document'] = train_data['document'].str.replace('^ +', \"\")  # change the white space to empty value\n",
            "<ipython-input-8-9d4d4d064f29>:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data['document'] = train_data['document'].str.replace('^ +', \"\")  # change the white space to empty value\n",
            "<ipython-input-8-9d4d4d064f29>:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data['document'].replace('', np.nan, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id            0\n",
            "document    789\n",
            "label         0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-9d4d4d064f29>:56: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  test_data['document'] = self.test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")  # Apply regular expression\n",
            "<ipython-input-8-9d4d4d064f29>:56: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data['document'] = self.test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")  # Apply regular expression\n",
            "<ipython-input-8-9d4d4d064f29>:59: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  test_data['document'] = self.test_data['document'].str.replace('^ +', \"\")  # change the white space to empty value\n",
            "<ipython-input-8-9d4d4d064f29>:59: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data['document'] = self.test_data['document'].str.replace('^ +', \"\")  # change the white space to empty value\n",
            "<ipython-input-8-9d4d4d064f29>:60: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data['document'].replace('', np.nan, inplace=True)  # change epmty space to null value\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "id          0\n",
            "document    1\n",
            "label       0\n",
            "dtype: int64\n",
            "The number of test dataset after preprocess : 49158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from konlpy.tag import Mecab\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "sf2Irfm_pHid"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize\n",
        "class Tokenize:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = Tokenizer()\n",
        "        self.stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다']\n",
        "        self.threshold = 3\n",
        "        self.total_cnt = len(self.tokenizer.word_index) # 단어의 수\n",
        "        self.rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "        self.total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "        self.rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "    def Tokenizer(self, data):\n",
        "        return self.tokenizer.fit_on_texts(data)\n",
        "\n",
        "\n",
        "    def pairing(self):\n",
        "        # 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "        for key, value in self.tokenizer.word_counts.items():\n",
        "            self.total_freq = self.total_freq + value\n",
        "\n",
        "            # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "            if(value < self.threshold):\n",
        "                self.rare_cnt = self.rare_cnt + 1\n",
        "                self.Tokenizerrare_freq = self.rare_freq + value\n",
        "\n",
        "        print('단어 집합(vocabulary)의 크기 :',self.total_cnt)\n",
        "        print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(self.threshold - 1, self.rare_cnt))\n",
        "        print(\"단어 집합에서 희귀 단어의 비율:\", (self.rare_cnt / self.total_cnt)*100)\n",
        "        print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (self.rare_freq / self.total_freq)*100)\n",
        "\n",
        "        ### 등장 빈도수 보다 작으면 제거\n",
        "        self.vocab_size = self.total_cnt - self.rare_cnt + 1\n",
        "        print('단어 집합의 크기 :',self.vocab_size)\n",
        "\n",
        "        return self.vocab_size\n",
        "\n",
        "        #\n",
        "    def text2int(self, data):\n",
        "        tokenizer = Tokenizer(self.vocab_size)\n",
        "        tokenizer.fit_on_texts(data)\n",
        "        if len(data) > 100000:\n",
        "            X_train = tokenizer.texts_to_sequences(data)\n",
        "            return X_train\n",
        "        else:\n",
        "            X_test = tokenizer.texts_to_sequences(data)\n",
        "            return X_test\n",
        "\n",
        "def below_threshold_len(max_len, nested_list):\n",
        "  count = 0\n",
        "  for sentence in nested_list:\n",
        "    if(len(sentence) <= max_len):\n",
        "        count = count + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    ### 클래스 선언\n",
        "    token = Tokenize()\n",
        "    data = preprocess()\n",
        "\n",
        "    ### 데이터 전처리\n",
        "    X_train, Y_train = data.process('train')\n",
        "    X_test, Y_test = data.process('test')\n",
        "\n",
        "    ### 불용어 제거\n",
        "    X_train = data.remove_stopword('train', X_train)\n",
        "    X_test = data.remove_stopword('test', X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUifZSxuk3sh",
        "outputId": "a997eb50-47bd-47d5-ea10-d97813d94fae"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-9d4d4d064f29>:36: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")\n",
            "<ipython-input-8-9d4d4d064f29>:36: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")\n",
            "<ipython-input-8-9d4d4d064f29>:40: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train_data['document'] = train_data['document'].str.replace('^ +', \"\")  # change the white space to empty value\n",
            "<ipython-input-8-9d4d4d064f29>:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data['document'] = train_data['document'].str.replace('^ +', \"\")  # change the white space to empty value\n",
            "<ipython-input-8-9d4d4d064f29>:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data['document'].replace('', np.nan, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id            0\n",
            "document    789\n",
            "label         0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-9d4d4d064f29>:56: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  test_data['document'] = self.test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")  # Apply regular expression\n",
            "<ipython-input-8-9d4d4d064f29>:56: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data['document'] = self.test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")  # Apply regular expression\n",
            "<ipython-input-8-9d4d4d064f29>:59: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  test_data['document'] = self.test_data['document'].str.replace('^ +', \"\")  # change the white space to empty value\n",
            "<ipython-input-8-9d4d4d064f29>:59: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data['document'] = self.test_data['document'].str.replace('^ +', \"\")  # change the white space to empty value\n",
            "<ipython-input-8-9d4d4d064f29>:60: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data['document'].replace('', np.nan, inplace=True)  # change epmty space to null value\n",
            "100%|██████████| 145393/145393 [13:42<00:00, 176.88it/s]\n",
            "100%|██████████| 48297/48297 [04:09<00:00, 193.58it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "# ### 토큰화 및 정수 인코딩\n",
        "    # train_token = token.Tokenizer(X_train)\n",
        "    # test_token = token.Tokenizer(X_test)\n",
        "\n",
        "    ### 등장 빈도 수 낮은 단어 제외\n",
        "    vocab_size = token.pairing()\n",
        "\n",
        "    ### text2sequence\n",
        "    train_token = token.text2int(X_train)\n",
        "    test_token = token.text2int(X_test)\n",
        "\n",
        "    ### remove empty sample\n",
        "    drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
        "    X_train = np.delete(X_train, drop_train, axis=0)\n",
        "    Y_train = np.delete(Y_train, drop_train, axis=0)\n",
        "\n",
        "    ### padding\n",
        "    max_len = 30\n",
        "    below_threshold_len(max_len, X_train)\n",
        "    X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "    X_test = pad_sequences(X_test, maxlen=max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "oAY1EhfOzgEZ",
        "outputId": "397d8b8a-42ea-4dfb-8dc7-1a9b82f46ae7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합(vocabulary)의 크기 : 0\n",
            "등장 빈도가 2번 이하인 희귀 단어의 수: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2e5933231099>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m### 등장 빈도 수 낮은 단어 제외\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m### text2sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-55ee3cb1858d>\u001b[0m in \u001b[0;36mpairing\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'단어 집합(vocabulary)의 크기 :'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_cnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'등장 빈도가 %s번 이하인 희귀 단어의 수: %s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrare_cnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"단어 집합에서 희귀 단어의 비율:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrare_cnt\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_cnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrare_freq\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch"
      ],
      "metadata": {
        "id": "-OLf9M-Vx80s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim, embedding_dim, dropout):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=0)\n",
        "    self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, dropout=dropout)\n",
        "    self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    embed = self.dropout(self.embedding(x))\n",
        "    output, _ = self.rnn(embed)\n",
        "    output = self.linear(output[:, -1, :])\n",
        "    output = nn.sigmoid(output)\n",
        "    return output\n",
        "\n",
        "  def _init_state(self, batch_size=1):\n",
        "    weight = next(self.parameters()).data\n",
        "    return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()"
      ],
      "metadata": {
        "id": "IsyhnNoYvq09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "batch_size = 5\n",
        "model = LSTM(input_dim=vocab_size, embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_dim=1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "train_iter = data.Iterator.splits((X_train), batch_size=batch_size, sort=False)"
      ],
      "metadata": {
        "id": "McYvBMPnx2iS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_iter):\n",
        "  model.train()\n",
        "\n",
        "  epoch_loss, epoch_acc = 0, 0\n",
        "  for batch in train_iter:\n",
        "    optimizer.zero_grad()\n",
        "    x, y = batch.text, batch.label\n",
        "    y_hat = model(x).squeeze(1)\n",
        "\n",
        "    loss = criterion(y_hat, y)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  return epoch_loss / len(train_iter), epoch_acc / len(train_iter)"
      ],
      "metadata": {
        "id": "v2SGoCTRxWre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _epoch in range(1,6): # 5 epoch\n",
        "  train_loss, train_acc = train(model, train_iter)"
      ],
      "metadata": {
        "id": "ZU5miP_c0Z6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "GX3199cQv1Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot()"
      ],
      "metadata": {
        "id": "17iwkbCVzEOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)\n",
        "\n"
      ],
      "metadata": {
        "id": "-YKrL4k4k3ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = load_model('best_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ],
      "metadata": {
        "id": "FBcWG6EUk3wc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}